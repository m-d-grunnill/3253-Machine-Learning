{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "W3_Homework.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "Os8UEIgdR24Y"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(123)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0z7V3gOR24c"
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1G_cA64R24d"
   },
   "source": [
    "Q1. Build a classification model for the default of credit card clients dataset. More info here:\n",
    "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "- Explore the data\n",
    "- Make sure you build a full data pipeline\n",
    "- Do you require any data pre-processing? Are all the features useful? (Use only raw features)\n",
    "- set the random seed to 123 (For splitting or any other random algorithm)\n",
    "- Split data into training (80%) and testing (20%)\n",
    "- Follow similar procedure as the one for week 2 (End-to-end Machine Learning Project). Remember apendix B\n",
    "- Study the ROC Curve, decide threshold\n",
    "- Use 2 classifiers.\n",
    "    - Random Forest\n",
    "        - tune only: n_estimators: {3, 4, 6, 7, 10, 20, 50, 100} \n",
    "    - KNN Classfier \n",
    "        - tune only: n_neighbors: {3, 4, 5, 7, 10, 20, 50} \n",
    "    - Which one performs better in the cross validation?\n",
    "    \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "- Cross-validation with 4-folds.\n",
    "\n",
    "- Other paramenters -> Use default\n",
    "\n",
    "Notes:\n",
    "  - Make your code modular, the second part of the assignmet you will have to repeat. \n",
    "  - Include documentation for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading data and preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jcK9rr6lR24d"
   },
   "source": [
    "### Your code here\n",
    "\n",
    "## Feel free to use multiple cells\n",
    "\n",
    "df = pd.read_excel(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\", \n",
    "                   sheet_name = 0, skiprows= 1, header = 0)\n",
    "df.head()"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n0   1      20000    2          2         1   24      2      2     -1     -1   \n1   2     120000    2          2         2   26     -1      2      0      0   \n2   3      90000    2          2         2   34      0      0      0      0   \n3   4      50000    2          2         1   37      0      0      0      0   \n4   5      50000    1          2         1   57     -1      0     -1      0   \n\n   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n0  ...          0          0          0         0       689         0   \n1  ...       3272       3455       3261         0      1000      1000   \n2  ...      14331      14948      15549      1518      1500      1000   \n3  ...      28314      28959      29547      2000      2019      1200   \n4  ...      20940      19146      19131      2000     36681     10000   \n\n   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n0         0         0         0                           1  \n1      1000         0      2000                           1  \n2      1000      1000      5000                           0  \n3      1100      1069      1000                           0  \n4      9000       689       679                           0  \n\n[5 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>LIMIT_BAL</th>\n      <th>SEX</th>\n      <th>EDUCATION</th>\n      <th>MARRIAGE</th>\n      <th>AGE</th>\n      <th>PAY_0</th>\n      <th>PAY_2</th>\n      <th>PAY_3</th>\n      <th>PAY_4</th>\n      <th>...</th>\n      <th>BILL_AMT4</th>\n      <th>BILL_AMT5</th>\n      <th>BILL_AMT6</th>\n      <th>PAY_AMT1</th>\n      <th>PAY_AMT2</th>\n      <th>PAY_AMT3</th>\n      <th>PAY_AMT4</th>\n      <th>PAY_AMT5</th>\n      <th>PAY_AMT6</th>\n      <th>default payment next month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>20000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>24</td>\n      <td>2</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>689</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>120000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>26</td>\n      <td>-1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3272</td>\n      <td>3455</td>\n      <td>3261</td>\n      <td>0</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>90000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>14331</td>\n      <td>14948</td>\n      <td>15549</td>\n      <td>1518</td>\n      <td>1500</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>5000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>50000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>28314</td>\n      <td>28959</td>\n      <td>29547</td>\n      <td>2000</td>\n      <td>2019</td>\n      <td>1200</td>\n      <td>1100</td>\n      <td>1069</td>\n      <td>1000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>50000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>57</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>20940</td>\n      <td>19146</td>\n      <td>19131</td>\n      <td>2000</td>\n      <td>36681</td>\n      <td>10000</td>\n      <td>9000</td>\n      <td>689</td>\n      <td>679</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "49940"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['BILL_AMT6'].iloc[3330]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET: default payment next month\n",
      "columns_predictors: ['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
      "columns_categorical: ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
      "columns_numerical: ['ID', 'LIMIT_BAL', 'AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n"
     ]
    }
   ],
   "source": [
    "TARGET = \"default payment next month\"\n",
    "columns_predictors = [col for col in df.columns if col not in [TARGET]]\n",
    "columns_categorical = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "columns_numerical = [col for col in columns_predictors if col not in columns_categorical]\n",
    "print(f\"TARGET: {TARGET}\")\n",
    "print(f\"columns_predictors: {columns_predictors}\")\n",
    "print(f\"columns_categorical: {columns_categorical}\")\n",
    "print(f\"columns_numerical: {columns_numerical}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df[columns_predictors]\n",
    "y = df[TARGET]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline #each pipeline can have multiple steps\n",
    "from sklearn.compose import ColumnTransformer #for selecting specific columns and transforming them with individual pipelines\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder #specific transformations we want to use in our pipelines\n",
    "\n",
    "pipeline_categorical = Pipeline([\n",
    "  ('onehot', OneHotEncoder()),\n",
    "])\n",
    "\n",
    "pipeline_numerical = Pipeline([\n",
    "  ('scaler', MinMaxScaler(feature_range=(0,1))),\n",
    "])\n",
    "\n",
    "pipeline_full = ColumnTransformer([\n",
    "  (\"categorical\", pipeline_categorical, columns_categorical),\n",
    "  (\"numerical\", pipeline_numerical, columns_numerical),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-test split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (22500, 24)\n",
      "X_test.shape: (7500, 24)\n",
      "y_train.shape: (22500,)\n",
      "y_test.shape: (7500,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y) #stratified sampling based on the target\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_test.shape: {X_test.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\"y_test.shape: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "pipeline_full.fit(X_train)\n",
    "X_train_transformed = pipeline_full.transform(X_train)\n",
    "X_test_transformed = pipeline_full.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11.8 s\n",
      "Wall time: 21.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "RandomForestClassifier()",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_transformed, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.7638290734192902\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "y_pred_proba = model.predict_proba(X_test_transformed)\n",
    "print(f\"roc_auc_score: {roc_auc_score(y_test, y_pred_proba[:,1])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is the impact of your AUC when you increase your training dataset from 0.6 to 0.7 and 0.8. Report the AUC for Random Forest model with different split sizes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "{'Training size 0.6: AUC score': 0.7667839152301271,\n 'Training size 0.7: AUC score': 0.776034207006146,\n 'Training size 0.8: AUC score': 0.7740426613402749}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for training_size in [0.6,0.7,0.8]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=training_size, stratify=y)\n",
    "    pipeline_full.fit(X_train)\n",
    "    X_train_transformed = pipeline_full.transform(X_train)\n",
    "    X_test_transformed = pipeline_full.transform(X_test)\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test_transformed)\n",
    "    results['Training size ' + str(training_size) +': AUC score'] = roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "\n",
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3HDtUyzR24j"
   },
   "source": [
    "#### Conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzyM49VVR24k"
   },
   "source": [
    "Explain your results and choices"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uZNssFNgR24l"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-6LCux9R24o"
   },
   "source": [
    "Q2. (Optional) Write a function that can shift an MNIST image in any direction (left, right, up, or down) by one pixel. Then, for each image in the training set, create four shifted copies (one per direction) and add them to the training set. Finally, train your best model on this expanded training set and measure its accuracy on the test set. You should observe that your model performs even better now! This technique of artificially growing the training set is called data augmentation or training set expansion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5UGBa2AR24o"
   },
   "source": [
    "### Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hBBG-JBHR24o"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}
